{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_predict_proba_lr',\n",
       " 'decision_function',\n",
       " 'densify',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'sparsify']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C hyper paramters is a regularization parameter that control how closely the model fits to the training data\n",
    "* C = 1/ lambda where lambda is the regularization parmeter.\n",
    "\n",
    "* When lamda is zero, then C will be close to infinity(Low Regularization -> High complexity -> Overfit)\n",
    "* When lambda is high, C will be less (High Regularization -> Low complexity -> Underfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* k fold cross validation will take your dataset and split in to k subsets.\n",
    "* It will iterate through k subsets k times\n",
    "* On each loop it will fit a model(train) k-1 subset and test on the remaining subset\n",
    "* Generate performance metrics for each loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib # pickle the model and save it\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV # use for hyper parmeter tuning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "\n",
    "# if we dont put header = None, then automatically it will assume that the first record will be the header\n",
    "train_labels = pd.read_csv('train_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "def print_results(results):\n",
    "    print(\"Best params is {}\".format(results.best_params_))\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "          print('{} (+/-){} for {}'.format(round(mean, 3), round(std *2, 3), params))\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params is {'C': 10}\n",
      "0.674 (+/-)0.083 for {'C': 0.001}\n",
      "0.706 (+/-)0.104 for {'C': 0.01}\n",
      "0.8 (+/-)0.115 for {'C': 0.1}\n",
      "0.801 (+/-)0.126 for {'C': 1}\n",
      "0.803 (+/-)0.118 for {'C': 10}\n",
      "0.8 (+/-)0.113 for {'C': 100}\n",
      "0.8 (+/-)0.113 for {'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "# cv=5 indicates that there dataset will be subset to 5\n",
    "cv = GridSearchCV(logistic_regression, parameters, cv=5)\n",
    "\n",
    "# values are now n vector form\n",
    "# we convert it to array using shape.ravel()\n",
    "cv.fit(train_features, train_labels.values.ravel())\n",
    "print_results(cv)\n",
    "\n",
    "# The above code will basically take the first parameter (0.001) and pass it through Logistic regression\n",
    "# Here we are doing 5 fold cross validation which will loop through 5 times 4 for training and one for testing\n",
    "# Average test score for the loop will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above results shows that when C is very low(which means high reegularization), then models are not working well because of Underfitting.\n",
    "\n",
    "#### Similarly when C is high(which means low regularization), models are not performing well because of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows us that the model gives good performance when c = 10\n",
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_model.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will save the model with the fit data which can be used for future purposes\n",
    "joblib.dump(cv.best_estimator_, 'LR_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
